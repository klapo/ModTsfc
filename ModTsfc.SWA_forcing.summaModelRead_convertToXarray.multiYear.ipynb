{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read SUMMA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# netcdf/numpy/xray/stats\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "# OS interaction\n",
    "import sys, pickle, os\n",
    "\n",
    "# import plotting\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import subplots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import kgraph\n",
    "\n",
    "# Offline Turbulence Package\n",
    "import turbpy\n",
    "\n",
    "# Customize\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context('talk')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Directory Lists\n",
    "# Unix\n",
    "if 'linux' in sys.platform:\n",
    "    dirPre = '/home/lapok/gdrive/'\n",
    "# Mac\n",
    "elif 'darwin' in sys.platform:\n",
    "    dirPre = '/Users/karllapo/gdrive/'\n",
    "\n",
    "dirProj = dirPre + 'SnowHydrology/proj/ModTsfc/'\n",
    "dirPrint = dirProj + 'Graphics'\n",
    "dirData = dirProj + 'data'\n",
    "dirSumma = dirProj + 'summa/ModTsfc_summa/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to quickly evaluate all model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(dirData)\n",
    "SWA = xr.open_dataset('SWA.ModTsfc.nc')\n",
    "SNQ = xr.open_dataset('SNQ.ModTsfc.nc')\n",
    "CDP = xr.open_dataset('CDP.ModTsfc.nc')\n",
    "\n",
    "SWA = SWA.loc[dict(time = slice(datetime(2006, 10, 1), datetime(2012, 9, 30, 23, 0, 0)))]\n",
    "SWA_daily = SWA.resample(how='mean', freq='d', dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define control lists etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siteNames = ['SWA', 'CDP']\n",
    "siteYears = {'SWA': np.arange(2006, 2012),\n",
    "             'CDP': np.arange(2002, 2009)}\n",
    "experiments = ['longwaveBias', 'shortwaveBias', 'specHumidBias',\n",
    "               'airTempBias', 'windBias']\n",
    "\n",
    "# ----------\n",
    "# Experiment names - Met Uncertainty\n",
    "metName = {'longwaveBias': 'LWRadAtm',\n",
    "           'shortwaveBias': 'SWRadAtm',\n",
    "           'specHumidBias': 'spechum',\n",
    "           'airTempBias': 'airtemp',\n",
    "           'windBias': 'windspd'}\n",
    "\n",
    "# Experiment IDs and values\n",
    "expNames = {'longwaveBias': ['lowLW_L','lowLW_S', 'highLW_S', 'highLW_L']}\n",
    "expBias = {'longwaveBias': {'lowLW_L': -25, 'lowLW_S': -10, 'highLW_S': 10, 'highLW_L': 25}}\n",
    "\n",
    "expNames['shortwaveBias'] = ['lowSW_L','lowSW_S', 'highSW_S', 'highSW_L']\n",
    "expBias['shortwaveBias'] = {'lowSW_L': -80, 'lowSW_S': -30, 'highSW_S': 30, 'highSW_L': 80}\n",
    "\n",
    "expNames['specHumidBias'] = ['lowQS_L','lowQS_S', 'highQS_S', 'highQS_L']\n",
    "expBias['specHumidBias'] = {'lowQS_L': -25,  'lowQS_S': -10, 'highQS_S': 10, 'highQS_L': 25}\n",
    "\n",
    "expNames['airTempBias'] = ['lowTair_L','lowTair_S', 'highTair_S', 'highTair_L']\n",
    "expBias['airTempBias'] = {'lowTair_L': -3, 'lowTair_S': -1.5, 'highTair_S': 1.5, 'highTair_L': 3}\n",
    "\n",
    "expNames['windBias'] = ['lowU_L','lowU_S', 'highU_S', 'highU_L']\n",
    "expBias['windBias'] = {'lowU_L': -3, 'lowU_S': -1.5, 'highU_S': 1.5, 'highU_L': 3}\n",
    "\n",
    "expNames['Obs'] = 'Obs'\n",
    "expBias['Obs'] = 0.\n",
    "\n",
    "# ----------\n",
    "# Scalar variables to extract from output\n",
    "scalar_data_vars = ['scalarRainPlusMelt', 'scalarSWE', 'scalarSnowSublimation', \n",
    "                    'scalarInfiltration', 'scalarSurfaceRunoff', 'scalarSurfaceTemp',\n",
    "                    'scalarSenHeatTotal', 'scalarLatHeatTotal', 'scalarSnowDepth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "longwaveBias\n",
      "\n",
      "lowLW_L_2011-2012_lowLW_L_1.nc appears to be corrupted, skipping...\n",
      "lowLW_S_2011-2012_lowLW_S_1.nc appears to be corrupted, skipping...\n",
      "\n",
      "shortwaveBias\n",
      "\n",
      "\n",
      "specHumidBias\n",
      "\n",
      "lowQS_L_2011-2012_lowQS_L_1.nc appears to be corrupted, skipping...\n",
      "\n",
      "airTempBias\n",
      "\n",
      "\n",
      "windBias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put all results in a single dictionary\n",
    "resultsDict = {}\n",
    "sN = 'SWA'\n",
    "for exp in experiments:\n",
    "    os.chdir(dirSumma + exp + '/' + sN)\n",
    "    print('')\n",
    "    print(exp)\n",
    "    print('')        \n",
    "    for names in expNames[exp]:\n",
    "        expName = names\n",
    "        for year in siteYears[sN]:\n",
    "            wy = str(year) + '-' + str(year + 1) \n",
    "            fileName = expName + '_' + wy + '_' + expName + '_1.nc'\n",
    "\n",
    "            try:\n",
    "                resultsDict[expName + 'wy' + str(year + 1)] = xr.open_dataset(fileName)\n",
    "            except OSError:\n",
    "                print(fileName + ' was not found, skipping...')\n",
    "            except IndexError:\n",
    "                print(fileName + ' appears to be corrupted, skipping...')\n",
    "\n",
    "# for r in resultsDict:\n",
    "#     resultsDict[r]['time'] = SWA.time.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultsDataSet = xr.Dataset()\n",
    "tempDict = {}\n",
    "\n",
    "for expNum, exp in enumerate(experiments):\n",
    "    for nameNum, names in enumerate(expNames[exp]):\n",
    "        tempDataSet = xr.Dataset()\n",
    "        for k in scalar_data_vars:\n",
    "            if not 'scalar' in k:\n",
    "                continue\n",
    "\n",
    "            # Assign scalar value to a dataArray\n",
    "            try:\n",
    "                darray = xr.concat([resultsDict[scen][k] for scen in sorted(resultsDict)\n",
    "                                    if names + 'wy' in scen], dim='time')\n",
    "                # Remove hru dimension, as it is unused\n",
    "                darray = darray.squeeze('hru')\n",
    "                darray = darray.drop('hru')\n",
    "                tempDataSet[k] = darray\n",
    "            except ValueError:\n",
    "                print('Could not find ' + names)\n",
    "        # Assign to a temporary dictionary for each experiment name\n",
    "        tempDataSet.coords['exp'] = exp\n",
    "        tempDataSet.coords['family'] = names.split('_')[0]\n",
    "        tempDict[names] = tempDataSet\n",
    "    \n",
    "resultsDataSet = xr.concat([tempDict[scen] for exp in experiments for scen in expNames[exp]], dim='expID')\n",
    "resultsDataSet.coords['expID'] = list([n for exp in experiments for n in expNames[exp] ])\n",
    "\n",
    "# Recreate time series since summa's time format does not play well with datetime\n",
    "resultsDataSet['time'] = pd.date_range(datetime(2006, 10, 1), datetime(2012, 9, 30, 23, 0, 0), freq='H')\n",
    "\n",
    "os.chdir(dirData + '/summaResults')\n",
    "resultsDataSet.to_netcdf('SWA.summaResults.forcing.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold content and dU/dt\n",
    "- somehow messed up the Model_Output.txt. None of the necessary variables were saved\n",
    "- Proceed using just surfact temperature for the time being\n",
    "- Dammit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ds = resultsDict['jrdn1991wy2007']\n",
    "# ds = ds.squeeze('hru')\n",
    "# Toffset = 273.15\n",
    "\n",
    "# # get the time information so we can plot centered on the time step\n",
    "# time = ds.time.values\n",
    "# time = time - 0.5 * (time[1] - time[0]) # plot bars centered on time step\n",
    "\n",
    "# # get the number of layers and the maximum\n",
    "# layers = ds.nLayers.values.astype('int') \n",
    "# max_layers = layers.max()\n",
    "# soilLayers = ds.nSoil.mean(dim='time').values.astype('int')\n",
    "# snowLayers = max_layers - soilLayers\n",
    "\n",
    "# # Set up two arrays that contain the layer information per timestep\n",
    "# depths = np.empty((max_layers + 1, len(time))) * np.nan\n",
    "# tmp = np.empty((max_layers, len(time))) * np.nan\n",
    "# coldContent = np.empty((len(time))) * np.nan\n",
    "\n",
    "# # extract the information from the SUMMA output and store it in the arrays\n",
    "# for i in range(len(time)):\n",
    "#     ifcstart = int(ds.ifcTotoStartIndex[i].values - 1)\n",
    "#     midstart = int(ds.midTotoStartIndex[i].values - 1)\n",
    "#     ifclayers = layers[i] + 1\n",
    "#     midlayers = layers[i]\n",
    "#     ifcend = int(ifcstart + ifclayers)\n",
    "#     midend = int(midstart + midlayers)\n",
    "#     depths[0:ifclayers, i] = -ds.iLayerHeight[ifcstart:ifcend]\n",
    "#     tmp[0:midlayers, i] = ds.mLayerTemp[midstart:midend] - Toffset\n",
    "# #     if ds.scalarSnowDepth > 0:\n",
    "# #         coldContent = tmp[:snowLayers, i] \n",
    "\n",
    "# # # now plot it\n",
    "# # import matplotlib as mpl\n",
    "# # fig = plt.figure(figsize=(15, 10)) # set up the plot\n",
    "# # colors = plt.get_cmap('RdYlBu_r') # select an appropriate color map\n",
    "# # norm = mpl.colors.Normalize(vmin=-3, vmax=3) # scale the color bar\n",
    "# # width = (time[1]-time[0])/np.timedelta64(1, 'D') # determine the width of the bar\n",
    "# # prev = depths[0] # this is used as the \"bottom\" for the bars\n",
    "# # # loop over each of the layers\n",
    "# # for s, w in zip(depths[1:], tmp[0:]):\n",
    "# #     plt.bar(time, prev-s, width=width, color=colors(norm(w)), bottom=s, edgecolor='none')\n",
    "# #     prev = s\n",
    "    \n",
    "# # # plot a color bar\n",
    "# # sm = plt.cm.ScalarMappable(cmap=colors, norm=norm)\n",
    "# # sm._A = []\n",
    "# # plt.colorbar(sm, extend='both', label='{} ({})'.format(ds.mLayerTemp.long_name, 'ÂºC'))\n",
    "# # # add a top line with the snow depth\n",
    "# # plt.plot(time, (ds.iLayerHeight[ds.ifcTotoStartIndex-1]*(-1)).values, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(depths[:int(ds.nLayers.values[4000]) - int(ds.nSoil.values[4000]), 4000])\n",
    "# plt.plot(depths[:, 4000])\n",
    "# plt.plot(tmp[:, 4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
